// Enhanced Multimodal Emotion AI System - Voice & Text Context Analysis
class EnhancedMultimodalEmotionAI {
    constructor() {
        // Enhanced emotion data from provided JSON
        this.primaryEmotions = ["joy", "sadness", "anger", "fear", "surprise", "disgust", "neutral"];
        this.complexEmotions = ["excitement", "frustration", "anxiety", "confidence", "boredom", "curiosity", "embarrassment", "pride", "guilt", "shame", "contempt", "admiration", "relief", "anticipation", "love", "hate", "nostalgia", "envy"];
        this.allEmotions = [...this.primaryEmotions, ...this.complexEmotions];

        // Enhanced 8-Model Architecture with Multimodal Capabilities
        this.enhancedModels = [
            {
                name: "MegaBERT", specialty: "Complex semantic emotions", color: "#4F46E5", icon: "üß†",
                audioAnalysis: "Vocal complexity, sophisticated emotional undertones, subtle prosodic patterns",
                textAnalysis: "Deep semantic analysis, literary emotional patterns, contextual emotional intelligence",
                fusionStrength: "Semantic sophistication + vocal sophistication",
                accuracy: 94.2,
                strengths: ["nostalgia", "contempt", "admiration", "love", "hate"]
            },
            {
                name: "EmotionRoBERTa", specialty: "General emotion detection", color: "#059669", icon: "üé≠",
                audioAnalysis: "General prosodic emotion indicators, vocal energy patterns, standard emotional markers",
                textAnalysis: "Emotion-specific word patterns, sentiment indicators, emotional expressions",
                fusionStrength: "Balanced vocal and textual emotion signals",
                accuracy: 96.8,
                strengths: ["joy", "anger", "sadness", "fear", "surprise"]
            },
            {
                name: "PsychoDistilBERT", specialty: "Psychological states", color: "#DC2626", icon: "üßÆ",
                audioAnalysis: "Vocal stress indicators, anxiety markers, depression signals, psychological vocal patterns",
                textAnalysis: "Mental health language patterns, cognitive behavioral indicators, psychological terminology",
                fusionStrength: "Clinical-grade psychological assessment",
                accuracy: 92.5,
                strengths: ["anxiety", "guilt", "shame", "pride", "embarrassment"]
            },
            {
                name: "AdvancedHuBERT", specialty: "Acoustic emotions", color: "#7C2D12", icon: "üéµ",
                audioAnalysis: "Advanced acoustic features, micro-expressions, vocal fold patterns, breathing rhythm",
                textAnalysis: "Contextual understanding of what emotions the acoustic patterns might represent",
                fusionStrength: "World-class acoustic analysis + semantic understanding",
                accuracy: 95.7,
                strengths: ["joy", "anger", "sadness", "excitement", "fear"]
            },
            {
                name: "UltraWav2Vec", specialty: "Audio features", color: "#1D4ED8", icon: "üîä",
                audioAnalysis: "Audio waveform analysis, energy distribution, vocal intensity, spectral features",
                textAnalysis: "Energy-related words, activation level indicators, arousal descriptors",
                fusionStrength: "Audio energy + textual arousal indicators",
                accuracy: 93.4,
                strengths: ["excitement", "boredom", "confidence", "anticipation", "relief"]
            },
            {
                name: "EmotionVOSK", specialty: "Speech patterns", color: "#BE185D", icon: "üó£Ô∏è",
                audioAnalysis: "Speech pattern analysis, hesitations, speech rate, vocal fry, uptalk patterns",
                textAnalysis: "Linguistic markers, questioning patterns, exclamations, speech-to-text emotional cues",
                fusionStrength: "Speech pattern analysis + linguistic understanding",
                accuracy: 91.8,
                strengths: ["surprise", "curiosity", "frustration", "neutral", "anticipation"]
            },
            {
                name: "NuanceGPT", specialty: "Contextual emotions", color: "#059669", icon: "üí°",
                audioAnalysis: "Subtle vocal nuances, undertones, implied emotions, contextual vocal cues",
                textAnalysis: "Subtext analysis, implied meanings, contextual emotional intelligence, nuanced language",
                fusionStrength: "Master of subtle emotion detection",
                accuracy: 97.2,
                strengths: ["embarrassment", "pride", "relief", "anticipation", "guilt"]
            },
            {
                name: "TemporalLSTM", specialty: "Temporal patterns", color: "#7C2D12", icon: "‚è∞",
                audioAnalysis: "Temporal vocal patterns, emotion progression, vocal stability over time",
                textAnalysis: "Narrative flow, emotional arc, temporal linguistic patterns",
                fusionStrength: "Emotional journey tracking through voice and text progression",
                accuracy: 94.8,
                strengths: ["neutral", "anxiety", "confidence", "boredom", "transitions"]
            }
        ];

        // Evidence types for detailed explanations
        this.evidenceTypes = ["audio_evidence", "text_evidence", "contextual_evidence", "temporal_evidence", "multimodal_fusion"];

        // Emotion colors and icons
        this.emotionColors = {
            "joy": "#10B981", "anger": "#EF4444", "sadness": "#3B82F6", "fear": "#8B5CF6",
            "surprise": "#F59E0B", "disgust": "#84CC16", "neutral": "#6B7280", "excitement": "#F59E0B",
            "frustration": "#DC2626", "anxiety": "#7C3AED", "confidence": "#059669", "boredom": "#9CA3AF",
            "curiosity": "#0891B2", "embarrassment": "#EC4899", "pride": "#D97706", "guilt": "#7C2D12",
            "shame": "#991B1B", "contempt": "#374151", "admiration": "#0D9488", "relief": "#65A30D",
            "anticipation": "#2563EB", "love": "#BE185D", "hate": "#7F1D1D", "nostalgia": "#8B5A2B", "envy": "#166534"
        };

        this.emotionIcons = {
            "joy": "üòä", "anger": "üò†", "sadness": "üò¢", "fear": "üò®", "surprise": "üòÆ", "disgust": "ü§¢",
            "neutral": "üòê", "excitement": "ü§©", "frustration": "üò§", "anxiety": "üò∞", "confidence": "üòé",
            "boredom": "üòë", "curiosity": "ü§î", "embarrassment": "üò≥", "pride": "ü¶ö", "guilt": "üòî",
            "shame": "üôà", "contempt": "üôÑ", "admiration": "üòç", "relief": "üòå", "anticipation": "ü§û",
            "love": "‚ù§Ô∏è", "hate": "üíî", "nostalgia": "üåÖ", "envy": "üòí"
        };

        // System state
        this.isRecording = false;
        this.recordingStartTime = null;
        this.recordingTimer = null;
        this.transcriptTimer = null;
        this.currentAnalysisResult = null;
        this.analysisHistory = [];
        this.currentTranscript = "";
        this.transcriptSegments = [];

        // Overall system accuracy
        this.overallAccuracy = 98.9;

        this.initializeSystem();
    }

    initializeSystem() {
        this.setupEventListeners();
        console.log('üé≠ Enhanced Multimodal Emotion AI System Initialized');
        console.log('üöÄ 8-Model Architecture with Voice + Text Analysis Active');
        console.log('üìù Real-Time Transcription Ready');
        console.log('üîó Cross-Modal Evidence Correlation Enabled');
    }

    setupEventListeners() {
        // Audio controls
        document.addEventListener('click', (e) => {
            if (e.target.id === 'recordBtn' || e.target.closest('#recordBtn')) {
                e.preventDefault();
                this.toggleRecording();
            } else if (e.target.id === 'uploadBtn' || e.target.closest('#uploadBtn')) {
                e.preventDefault();
                this.triggerFileUpload();
            } else if (e.target.id === 'exportMultimodalBtn') {
                e.preventDefault();
                this.exportCompleteAnalysis();
            } else if (e.target.id === 'viewDetailsBtn') {
                e.preventDefault();
                this.showDetailedEvidence();
            } else if (e.target.id === 'newAnalysisBtn') {
                e.preventDefault();
                this.resetForNewAnalysis();
            } else if (e.target.id === 'closeEvidenceModal') {
                e.preventDefault();
                this.closeEvidenceModal();
            } else if (e.target.id === 'clearHistoryBtn') {
                e.preventDefault();
                this.clearAnalysisHistory();
            } else if (e.target.id === 'exportHistoryBtn') {
                e.preventDefault();
                this.exportAnalysisHistory();
            }
        });

        // File upload
        const audioUpload = document.getElementById('audioUpload');
        if (audioUpload) {
            audioUpload.addEventListener('change', (e) => this.handleFileUpload(e));
        }

        // Modal controls
        const evidenceModal = document.getElementById('evidenceModal');
        if (evidenceModal) {
            evidenceModal.addEventListener('click', (e) => {
                if (e.target.id === 'evidenceModal') {
                    this.closeEvidenceModal();
                }
            });
        }
    }

    toggleRecording() {
        const recordBtn = document.getElementById('recordBtn');
        const recordText = document.getElementById('recordText');
        const recordingStatus = document.getElementById('recordingStatus');
        const liveTranscript = document.getElementById('liveTranscript');

        if (!recordBtn || !recordText || !recordingStatus || !liveTranscript) {
            console.error('Recording UI elements not found');
            return;
        }

        if (!this.isRecording) {
            this.startRecording();
        } else {
            this.stopRecording();
        }
    }

    startRecording() {
        this.isRecording = true;
        this.recordingStartTime = Date.now();
        this.currentTranscript = "";
        this.transcriptSegments = [];

        // Update UI elements
        const recordBtn = document.getElementById('recordBtn');
        const recordText = document.getElementById('recordText');
        const recordingStatus = document.getElementById('recordingStatus');
        const liveTranscript = document.getElementById('liveTranscript');
        const recordTimeElement = document.getElementById('recordTime');
        const transcriptText = document.getElementById('transcriptText');

        if (recordText) recordText.textContent = 'üõë Stop Recording & Analyze';
        if (recordBtn) {
            recordBtn.classList.remove('btn--primary');
            recordBtn.classList.add('btn--secondary');
        }
        if (recordingStatus) recordingStatus.classList.remove('hidden');
        if (liveTranscript) liveTranscript.classList.remove('hidden');
        if (recordTimeElement) recordTimeElement.textContent = '00:00';
        if (transcriptText) transcriptText.textContent = 'Starting transcription...';

        // Start recording timer
        this.recordingTimer = setInterval(() => {
            if (!this.isRecording) {
                clearInterval(this.recordingTimer);
                return;
            }

            const elapsed = Math.floor((Date.now() - this.recordingStartTime) / 1000);
            const minutes = Math.floor(elapsed / 60).toString().padStart(2, '0');
            const seconds = (elapsed % 60).toString().padStart(2, '0');

            if (recordTimeElement) {
                recordTimeElement.textContent = `${minutes}:${seconds}`;
            }
        }, 1000);

        // Simulate real-time transcription
        this.simulateRealTimeTranscription();

        console.log('üéôÔ∏è Recording and transcription started');
    }

    simulateRealTimeTranscription() {
        const sampleTranscripts = [
            "I'm feeling really excited about this new project we're working on",
            "It's been a challenging day, but I'm trying to stay positive and focused",
            "I'm a bit nervous about the presentation tomorrow, but I think it will go well",
            "The weather is absolutely beautiful today, it really makes me feel happy and energized",
            "I'm quite frustrated with how things are going at work lately",
            "I feel very confident that we can solve this problem together as a team",
            "This whole situation is making me quite anxious and worried about the outcome",
            "I'm so grateful for all the support and encouragement I've received from everyone"
        ];

        const selectedTranscript = sampleTranscripts[Math.floor(Math.random() * sampleTranscripts.length)];
        const words = selectedTranscript.split(' ');
        let currentWordIndex = 0;

        this.transcriptTimer = setInterval(() => {
            if (!this.isRecording || currentWordIndex >= words.length) {
                if (this.transcriptTimer) {
                    clearInterval(this.transcriptTimer);
                    this.transcriptTimer = null;
                }
                return;
            }

            const transcriptText = document.getElementById('transcriptText');
            if (transcriptText) {
                const currentText = words.slice(0, currentWordIndex + 1).join(' ');
                transcriptText.textContent = currentText;
                this.currentTranscript = currentText;
            }

            currentWordIndex++;
        }, 600 + Math.random() * 400);
    }

    stopRecording() {
        this.isRecording = false;
        
        // Clear timers
        if (this.recordingTimer) {
            clearInterval(this.recordingTimer);
            this.recordingTimer = null;
        }
        if (this.transcriptTimer) {
            clearInterval(this.transcriptTimer);
            this.transcriptTimer = null;
        }

        // Update UI elements
        const recordBtn = document.getElementById('recordBtn');
        const recordText = document.getElementById('recordText');
        const recordingStatus = document.getElementById('recordingStatus');
        const liveTranscript = document.getElementById('liveTranscript');
        const recordTimeElement = document.getElementById('recordTime');

        if (recordText) recordText.textContent = 'üéôÔ∏è Start Recording & Transcription';
        if (recordBtn) {
            recordBtn.classList.remove('btn--secondary');
            recordBtn.classList.add('btn--primary');
        }
        if (recordingStatus) recordingStatus.classList.add('hidden');
        if (recordTimeElement) recordTimeElement.textContent = '00:00';

        console.log('üõë Recording stopped, starting enhanced multimodal analysis...');
        console.log('üìù Final transcript:', this.currentTranscript);

        // Start analysis after a brief delay
        setTimeout(() => {
            this.startEnhancedMultimodalAnalysis('recorded_audio.wav', this.currentTranscript);
        }, 1000);
    }

    triggerFileUpload() {
        const audioUpload = document.getElementById('audioUpload');
        if (audioUpload) {
            audioUpload.click();
        }
    }

    handleFileUpload(event) {
        const file = event.target.files[0];
        if (file) {
            console.log(`üìÅ File uploaded: ${file.name}`);
            
            // Generate a sample transcript for uploaded file
            const sampleTranscripts = [
                "I've been thinking about this decision for weeks now. It's really important to me and I want to make sure I get it right. There's a part of me that's excited about the possibilities, but I'm also feeling anxious about the potential risks.",
                "Today was such an incredible day! Everything just seemed to fall into place perfectly. I'm feeling so grateful and happy about how things turned out. This positive energy is exactly what I needed.",
                "I'm really struggling with this situation at work. It's been causing me a lot of stress and I'm not sure how to handle it. I feel frustrated and a bit overwhelmed by everything that's happening.",
                "The presentation went better than I expected! I was nervous beforehand, but once I started talking, I felt much more confident. I'm proud of how I handled the questions at the end."
            ];
            
            const selectedTranscript = sampleTranscripts[Math.floor(Math.random() * sampleTranscripts.length)];
            this.currentTranscript = selectedTranscript;
            
            // Hide live transcript section since this is file upload
            const liveTranscript = document.getElementById('liveTranscript');
            if (liveTranscript) liveTranscript.classList.add('hidden');
            
            this.startEnhancedMultimodalAnalysis(file.name, selectedTranscript);
        }
    }

    async startEnhancedMultimodalAnalysis(audioSource, transcript) {
        console.log('üîÑ Starting Enhanced Multimodal Analysis...');
        console.log('üé§ Audio Source:', audioSource);
        console.log('üìù Transcript:', transcript);

        // Reset all model progress bars and status
        this.resetModelStatus();

        // Start processing all models
        try {
            await this.processAllEnhancedModels();

            // Generate comprehensive multimodal results
            const analysisResult = this.generateEnhancedMultimodalResults(audioSource, transcript);
            this.currentAnalysisResult = analysisResult;

            console.log('‚úÖ Enhanced multimodal analysis complete');

            // Display results
            this.displayEnhancedMultimodalResults(analysisResult);

            // Add to history
            this.addToHistory(analysisResult);

            // Show results section
            const multimodalResults = document.getElementById('multimodalResults');
            if (multimodalResults) {
                multimodalResults.classList.remove('hidden');
                // Scroll to results
                multimodalResults.scrollIntoView({ behavior: 'smooth' });
            }

        } catch (error) {
            console.error('Enhanced multimodal analysis failed:', error);
            this.handleAnalysisError();
        }
    }

    resetModelStatus() {
        this.enhancedModels.forEach(model => {
            const statusElement = document.getElementById(`status-${model.name}`);
            const progressElement = document.getElementById(`progress-${model.name}`);
            
            if (statusElement) {
                statusElement.textContent = 'Ready';
                statusElement.classList.remove('processing', 'completed');
            }
            
            if (progressElement) {
                progressElement.style.width = '0%';
            }
        });
    }

    async processAllEnhancedModels() {
        console.log('üöÄ Processing all 8 enhanced models with multimodal capabilities...');

        // Process all models simultaneously
        const modelPromises = this.enhancedModels.map((model, index) => {
            return this.processEnhancedModel(model, index * 150 + 300);
        });

        await Promise.all(modelPromises);
        console.log('‚úÖ All enhanced models processed successfully');
    }

    async processEnhancedModel(model, delay) {
        return new Promise((resolve) => {
            setTimeout(() => {
                const progressElement = document.getElementById(`progress-${model.name}`);
                const statusElement = document.getElementById(`status-${model.name}`);

                if (statusElement) {
                    statusElement.textContent = 'Processing';
                    statusElement.classList.add('processing');
                }

                if (!progressElement) {
                    resolve();
                    return;
                }

                let progress = 0;
                const interval = setInterval(() => {
                    progress += Math.random() * 12 + 3;
                    progressElement.style.width = `${Math.min(progress, 100)}%`;

                    if (progress >= 100) {
                        clearInterval(interval);
                        if (statusElement) {
                            statusElement.textContent = '‚úì Complete';
                            statusElement.classList.remove('processing');
                            statusElement.classList.add('completed');
                        }

                        console.log(`‚úÖ ${model.name} enhanced multimodal processing complete`);
                        resolve();
                    }
                }, 40);
            }, delay);
        });
    }

    generateEnhancedMultimodalResults(audioSource, transcript) {
        // Generate comprehensive multimodal analysis results
        const results = {
            timestamp: new Date(),
            audioSource,
            transcript,
            overallAccuracy: this.overallAccuracy,
            transcriptConfidence: (Math.random() * 5 + 95).toFixed(1),
            processingTime: (Math.random() * 1.5 + 2.0).toFixed(1),
            
            // Generate transcript segments with emotion annotations
            transcriptSegments: this.generateTranscriptSegments(transcript),
            
            // Generate enhanced model results with evidence
            modelResults: this.generateEnhancedModelResults(transcript),
            
            // Generate emotional timeline
            emotionalTimeline: this.generateEmotionalTimeline(),
            
            // Generate cross-modal evidence correlation
            evidenceCorrelation: this.generateEvidenceCorrelation(transcript)
        };

        return results;
    }

    generateTranscriptSegments(transcript) {
        const sentences = transcript.split(/[.!?]+/).filter(s => s.trim().length > 0);
        const segments = [];

        sentences.forEach((sentence, index) => {
            const startTime = index * 3 + Math.random() * 2;
            const emotions = this.detectEmotionsInText(sentence.trim());
            
            segments.push({
                id: index,
                startTime: startTime.toFixed(1),
                endTime: (startTime + sentence.length * 0.1 + 2).toFixed(1),
                content: sentence.trim(),
                emotions: emotions.slice(0, 2) // Top 2 emotions per segment
            });
        });

        return segments;
    }

    detectEmotionsInText(text) {
        const emotionalKeywords = {
            joy: ['happy', 'excited', 'great', 'wonderful', 'amazing', 'love', 'fantastic', 'beautiful', 'incredible', 'perfect'],
            anxiety: ['nervous', 'worried', 'anxious', 'scared', 'concerned', 'uncertain', 'afraid', 'stress', 'overwhelmed'],
            anger: ['angry', 'frustrated', 'annoyed', 'mad', 'irritated', 'furious', 'struggling'],
            sadness: ['sad', 'disappointed', 'depressed', 'down', 'upset', 'hurt'],
            confidence: ['confident', 'sure', 'certain', 'positive', 'strong', 'ready', 'proud'],
            anticipation: ['excited', 'looking forward', 'anticipating', 'expecting', 'hoping'],
            relief: ['relieved', 'better', 'thankful', 'grateful', 'glad'],
            frustration: ['frustrating', 'difficult', 'challenging', 'problem', 'struggling'],
            excitement: ['excited', 'thrilled', 'energized', 'enthusiastic', 'amazing'],
            fear: ['afraid', 'scared', 'worried', 'nervous', 'anxious'],
            curiosity: ['thinking', 'wondering', 'considering', 'interesting', 'question']
        };

        const detectedEmotions = [];
        const lowerText = text.toLowerCase();

        Object.entries(emotionalKeywords).forEach(([emotion, keywords]) => {
            const matchCount = keywords.filter(keyword => lowerText.includes(keyword)).length;
            if (matchCount > 0) {
                detectedEmotions.push({
                    emotion,
                    confidence: Math.min(0.9, 0.3 + matchCount * 0.2),
                    keywords: keywords.filter(keyword => lowerText.includes(keyword))
                });
            }
        });

        // Add some default emotions if none detected
        if (detectedEmotions.length === 0) {
            detectedEmotions.push({
                emotion: 'neutral',
                confidence: 0.6,
                keywords: []
            });
        }

        return detectedEmotions.sort((a, b) => b.confidence - a.confidence);
    }

    generateEnhancedModelResults(transcript) {
        const modelResults = {};

        this.enhancedModels.forEach(model => {
            // Generate audio evidence
            const audioEvidence = this.generateAudioEvidence(model);
            
            // Generate text evidence
            const textEvidence = this.generateTextEvidence(model, transcript);
            
            // Generate detected emotions with evidence
            const emotions = this.generateModelEmotions(model, transcript);
            
            // Generate multimodal fusion explanation
            const fusionExplanation = this.generateFusionExplanation(model, audioEvidence, textEvidence);

            modelResults[model.name] = {
                model,
                audioEvidence,
                textEvidence,
                emotions,
                fusionExplanation,
                confidence: (Math.random() * 8 + model.accuracy - 5).toFixed(1),
                processingTime: (Math.random() * 0.5 + 0.8).toFixed(1)
            };
        });

        return modelResults;
    }

    generateAudioEvidence(model) {
        const audioEvidenceTemplates = {
            "MegaBERT": [
                "Detected subtle vocal complexity at 0:15-0:23 indicating sophisticated emotional processing",
                "Sophisticated emotional undertones present in vocal patterns throughout recording",
                "Complex prosodic patterns suggest nuanced emotional state beyond basic categories"
            ],
            "EmotionRoBERTa": [
                "General prosodic emotion indicators show consistent emotional signal at 0:12-0:18",
                "Vocal energy patterns align with detected emotional state",
                "Standard emotional markers present in voice pitch and rhythm variations"
            ],
            "PsychoDistilBERT": [
                "Vocal stress indicators detected at 0:08-0:14 suggest psychological activation",
                "Anxiety markers present in speech rate and vocal tremor analysis",
                "Psychological vocal patterns indicate underlying emotional processing"
            ],
            "AdvancedHuBERT": [
                "Advanced acoustic features show micro-expressions in vocal fold patterns",
                "Breathing rhythm analysis indicates emotional arousal at 0:05-0:11",
                "Vocal fold tension patterns suggest specific emotional activation"
            ],
            "UltraWav2Vec": [
                "Audio waveform analysis reveals energy distribution peaks at 0:10-0:16",
                "Vocal intensity patterns correlate with emotional activation levels",
                "Spectral features indicate arousal-based emotional response"
            ],
            "EmotionVOSK": [
                "Speech pattern analysis shows hesitations at 0:07-0:09 indicating uncertainty",
                "Vocal fry patterns detected suggesting contemplative emotional state",
                "Speech rate variations align with emotional processing patterns"
            ],
            "NuanceGPT": [
                "Subtle vocal nuances detected in undertone analysis throughout recording",
                "Implied emotional cues present in contextual vocal inflections",
                "Nuanced vocal patterns suggest complex underlying emotional state"
            ],
            "TemporalLSTM": [
                "Temporal vocal patterns show emotional progression from 0:00 to 0:20",
                "Vocal stability analysis indicates emotional consistency over time",
                "Progression patterns suggest emotional development throughout recording"
            ]
        };

        const templates = audioEvidenceTemplates[model.name] || ["Audio analysis completed with model-specific patterns detected"];
        return templates.slice(0, 2 + Math.floor(Math.random() * 2));
    }

    generateTextEvidence(model, transcript) {
        const words = transcript.toLowerCase().split(' ');
        
        const textEvidenceTemplates = {
            "MegaBERT": [
                `Deep semantic analysis of words "${words.slice(0, 3).join(', ')}" indicates complex emotional undertones`,
                "Literary emotional patterns suggest sophisticated cognitive-emotional processing",
                "Contextual emotional intelligence detected in semantic word relationships"
            ],
            "EmotionRoBERTa": [
                `Emotion-specific word patterns identified: "${words.filter(w => w.length > 4).slice(0, 2).join(', ')}"`,
                "Sentiment indicators present in linguistic emotional expressions",
                "Balanced emotional language patterns detected throughout text"
            ],
            "PsychoDistilBERT": [
                "Mental health language patterns indicate psychological awareness",
                `Clinical indicators found in words: "${words.filter(w => ['thinking', 'feeling', 'anxious', 'worried', 'stressed'].some(keyword => w.includes(keyword))).slice(0, 2).join(', ')}"`,
                "Cognitive behavioral linguistic markers present in text structure"
            ],
            "AdvancedHuBERT": [
                "Contextual acoustic interpretation enhanced by semantic understanding",
                "Text provides context for acoustic emotional patterns detected",
                "Semantic-acoustic correlation strengthens emotion detection confidence"
            ],
            "UltraWav2Vec": [
                `Energy-related descriptors identified: "${words.filter(w => ['excited', 'tired', 'energetic', 'energized'].some(keyword => w.includes(keyword))).slice(0, 2).join(', ')}"`,
                "Arousal level indicators present in linguistic activation terms",
                "Textual energy markers correlate with audio intensity analysis"
            ],
            "EmotionVOSK": [
                "Linguistic markers suggest questioning or uncertainty patterns",
                `Speech-to-text emotional cues found in expressions: "${words.slice(-3).join(', ')}"`,
                "Linguistic questioning patterns align with speech hesitation analysis"
            ],
            "NuanceGPT": [
                "Subtext analysis reveals implied emotional meanings beyond surface text",
                "Contextual emotional intelligence detected in nuanced language use",
                "Implied meanings suggest complex emotional state not explicitly stated"
            ],
            "TemporalLSTM": [
                "Narrative flow indicates emotional progression throughout text",
                "Temporal linguistic patterns show emotional development over time",
                "Emotional arc detected in narrative structure and word progression"
            ]
        };

        const templates = textEvidenceTemplates[model.name] || ["Text analysis completed with linguistic patterns detected"];
        return templates.slice(0, 2 + Math.floor(Math.random() * 2));
    }

    generateModelEmotions(model, transcript) {
        const emotions = {};
        
        // Base emotions on model strengths
        model.strengths.forEach(emotion => {
            emotions[emotion] = Math.random() * 0.4 + 0.4; // 0.4-0.8
        });

        // Add emotions based on text analysis
        const textEmotions = this.detectEmotionsInText(transcript);
        textEmotions.forEach(({ emotion, confidence }) => {
            if (this.allEmotions.includes(emotion)) {
                emotions[emotion] = Math.max(emotions[emotion] || 0, confidence * 0.8);
            }
        });

        // Add some random emotions
        const randomEmotions = this.allEmotions.filter(e => !Object.keys(emotions).includes(e));
        for (let i = 0; i < 2; i++) {
            const randomEmotion = randomEmotions[Math.floor(Math.random() * randomEmotions.length)];
            if (randomEmotion && !emotions[randomEmotion]) {
                emotions[randomEmotion] = Math.random() * 0.3;
            }
        }

        return emotions;
    }

    generateFusionExplanation(model, audioEvidence, textEvidence) {
        const fusionTemplates = [
            `${model.fusionStrength}: Audio signals (${(Math.random() * 20 + 70).toFixed(0)}%) + text markers (${(Math.random() * 20 + 75).toFixed(0)}%) = High confidence detection`,
            `Multimodal correlation: Voice patterns align with textual emotional indicators, strengthening overall confidence by ${(Math.random() * 15 + 20).toFixed(0)}%`,
            `Cross-modal validation: Audio and text evidence mutually reinforce detected emotional state`,
            `${model.name} fusion strength: Combined analysis provides ${(Math.random() * 10 + 85).toFixed(0)}% confidence in emotion detection`
        ];

        return fusionTemplates[Math.floor(Math.random() * fusionTemplates.length)];
    }

    generateEmotionalTimeline() {
        const timePoints = [];
        const duration = 20; // 20 seconds
        const numPoints = 8;

        for (let i = 0; i < numPoints; i++) {
            const time = (duration / numPoints) * i;
            const emotions = {};
            
            // Select 1-2 random emotions for each time point
            const selectedEmotions = this.allEmotions
                .sort(() => 0.5 - Math.random())
                .slice(0, 1 + Math.floor(Math.random() * 2));

            selectedEmotions.forEach(emotion => {
                emotions[emotion] = Math.random() * 0.6 + 0.2;
            });

            timePoints.push({
                timestamp: time.toFixed(1),
                emotions,
                dominantEmotion: selectedEmotions[0],
                intensity: Math.max(...Object.values(emotions))
            });
        }

        return timePoints;
    }

    generateEvidenceCorrelation(transcript) {
        const correlations = [];
        const detectedEmotions = this.detectEmotionsInText(transcript);

        detectedEmotions.slice(0, 4).forEach(({ emotion, confidence, keywords }) => {
            correlations.push({
                emotion,
                confidence: (confidence * 100).toFixed(0),
                audioEvidence: this.generateCorrelationAudioEvidence(emotion),
                textEvidence: keywords.length > 0 ? 
                    `Keywords detected: "${keywords.join(', ')}" indicate ${emotion}` :
                    `Semantic patterns in text suggest ${emotion} through contextual analysis`,
                contextualEvidence: this.generateContextualEvidence(emotion),
                temporalEvidence: `Emotion peaks at ${(Math.random() * 15 + 5).toFixed(1)}s with sustained presence`,
                multimodalFusion: `Voice patterns (${(Math.random() * 20 + 75).toFixed(0)}%) + text indicators (${(Math.random() * 20 + 70).toFixed(0)}%) = Combined confidence ${(confidence * 100).toFixed(0)}%`
            });
        });

        return correlations;
    }

    generateCorrelationAudioEvidence(emotion) {
        const audioEvidenceMap = {
            joy: "Elevated pitch, increased vocal energy, and laughter-adjacent vocal patterns detected",
            sadness: "Lowered pitch, decreased vocal energy, and trembling patterns in voice",
            anger: "Increased vocal intensity, harsh spectral features, and rapid speech patterns",
            fear: "Voice trembling, pitch instability, and shortened breath patterns detected",
            anxiety: "Vocal tension, increased speech rate, and hesitation patterns present",
            confidence: "Steady vocal tone, consistent energy levels, and assertive speech patterns",
            excitement: "High vocal energy, increased pitch variability, and rapid speech tempo"
        };

        return audioEvidenceMap[emotion] || `Voice patterns consistent with ${emotion} detected through acoustic analysis`;
    }

    generateContextualEvidence(emotion) {
        const contextualMap = {
            joy: "Topic discussion and situational context align with positive emotional expression",
            sadness: "Subject matter and conversational tone suggest reflective, melancholic state",
            anger: "Discussion content and vocal emphasis patterns indicate frustration or anger",
            fear: "Uncertainty in topic discussion correlates with vocal apprehension markers",
            anxiety: "Worried discussion topics align with anxious vocal and linguistic patterns",
            confidence: "Assertive language and vocal certainty align with confident emotional state",
            excitement: "Enthusiastic topic discussion matches energetic vocal expression patterns"
        };

        return contextualMap[emotion] || `Contextual analysis supports ${emotion} detection through situational indicators`;
    }

    displayEnhancedMultimodalResults(results) {
        console.log('üìä Displaying enhanced multimodal results');

        // Update metrics
        this.updateAnalysisMetrics(results);
        
        // Display transcript with annotations
        this.displayAnnotatedTranscript(results.transcriptSegments);
        
        // Display all model results with evidence
        this.displayAllModelsResults(results.modelResults);
        
        // Display emotional timeline
        this.displayEmotionalTimeline(results.emotionalTimeline);
        
        // Display evidence correlation
        this.displayEvidenceCorrelation(results.evidenceCorrelation);

        console.log('‚úÖ Enhanced multimodal results displayed successfully');
    }

    updateAnalysisMetrics(results) {
        const overallAccuracyElement = document.getElementById('overallAccuracy');
        const transcriptConfidenceElement = document.getElementById('transcriptConfidence');
        const processingTimeElement = document.getElementById('processingTime');

        if (overallAccuracyElement) overallAccuracyElement.textContent = `${results.overallAccuracy}%`;
        if (transcriptConfidenceElement) transcriptConfidenceElement.textContent = `${results.transcriptConfidence}%`;
        if (processingTimeElement) processingTimeElement.textContent = `${results.processingTime}s`;
    }

    displayAnnotatedTranscript(segments) {
        const container = document.getElementById('annotatedTranscript');
        if (!container) return;

        container.innerHTML = '';

        if (segments.length === 0) {
            container.innerHTML = '<p>No transcript segments available.</p>';
            return;
        }

        segments.forEach(segment => {
            const segmentElement = document.createElement('div');
            segmentElement.className = 'transcript-segment';

            segmentElement.innerHTML = `
                <div class="transcript-timestamp">${segment.startTime}s - ${segment.endTime}s</div>
                <div class="transcript-content">${segment.content}</div>
                <div class="emotion-annotations">
                    ${segment.emotions.map(({ emotion, confidence }) => `
                        <span class="emotion-tag" style="background-color: ${this.emotionColors[emotion] || '#6B7280'}">
                            ${this.emotionIcons[emotion]} ${emotion} (${(confidence * 100).toFixed(0)}%)
                        </span>
                    `).join('')}
                </div>
            `;

            container.appendChild(segmentElement);
        });
    }

    displayAllModelsResults(modelResults) {
        const container = document.getElementById('allModelsResults');
        if (!container) return;

        container.innerHTML = '';

        Object.entries(modelResults).forEach(([modelName, result]) => {
            const modelCard = document.createElement('div');
            modelCard.className = 'enhanced-model-result';

            const topEmotions = Object.entries(result.emotions)
                .sort(([,a], [,b]) => b - a)
                .slice(0, 3);

            modelCard.innerHTML = `
                <div class="enhanced-model-header">
                    <span class="model-icon">${result.model.icon}</span>
                    <div class="model-title-info">
                        <h5>${modelName} Enhanced</h5>
                        <div class="model-confidence">${result.confidence}% Confidence</div>
                    </div>
                </div>
                
                <div class="evidence-sections">
                    <div class="evidence-section">
                        <div class="evidence-title">üé§ Audio Evidence</div>
                        <div class="evidence-list">
                            ${result.audioEvidence.map(evidence => `
                                <div class="evidence-item">${evidence}</div>
                            `).join('')}
                        </div>
                    </div>
                    
                    <div class="evidence-section">
                        <div class="evidence-title">üìù Text Evidence</div>
                        <div class="evidence-list">
                            ${result.textEvidence.map(evidence => `
                                <div class="evidence-item">${evidence}</div>
                            `).join('')}
                        </div>
                    </div>
                    
                    <div class="evidence-section">
                        <div class="evidence-title">üîó Multimodal Fusion</div>
                        <div class="evidence-list">
                            <div class="evidence-item">${result.fusionExplanation}</div>
                        </div>
                    </div>
                </div>
                
                <div class="detected-emotions">
                    <h6>Detected Emotions:</h6>
                    ${topEmotions.map(([emotion, confidence]) => `
                        <div class="emotion-detection">
                            <div class="emotion-name-with-icon">
                                <span>${this.emotionIcons[emotion]}</span>
                                <span>${emotion}</span>
                            </div>
                            <div class="emotion-confidence-score">${(confidence * 100).toFixed(1)}%</div>
                        </div>
                    `).join('')}
                </div>
            `;

            container.appendChild(modelCard);
        });
    }

    displayEmotionalTimeline(timeline) {
        const container = document.getElementById('emotionalTimeline');
        if (!container) return;

        const timelineContainer = document.createElement('div');
        timelineContainer.className = 'timeline-container';

        timeline.forEach(point => {
            const timelinePoint = document.createElement('div');
            timelinePoint.className = 'timeline-point';

            const intensity = point.intensity;
            const barHeight = Math.max(20, intensity * 160);

            timelinePoint.innerHTML = `
                <div class="emotion-intensity-bar" style="height: ${barHeight}px; background-color: ${this.emotionColors[point.dominantEmotion] || '#6B7280'}"></div>
                <div class="timeline-emotion-label">${this.emotionIcons[point.dominantEmotion]} ${point.dominantEmotion}</div>
                <div class="timeline-timestamp">${point.timestamp}s</div>
            `;

            timelineContainer.appendChild(timelinePoint);
        });

        container.innerHTML = '';
        container.appendChild(timelineContainer);
    }

    displayEvidenceCorrelation(correlations) {
        const container = document.getElementById('evidenceCorrelation');
        if (!container) return;

        container.innerHTML = '';

        correlations.forEach(correlation => {
            const correlationCard = document.createElement('div');
            correlationCard.className = 'correlation-card';

            correlationCard.innerHTML = `
                <div class="correlation-header">
                    <div class="correlation-emotion">
                        ${this.emotionIcons[correlation.emotion]} ${correlation.emotion}
                    </div>
                    <div class="correlation-confidence">${correlation.confidence}%</div>
                </div>
                
                <div class="correlation-details">
                    <div class="correlation-type">
                        <div class="correlation-type-title">üé§ Audio Evidence</div>
                        <div class="correlation-evidence">${correlation.audioEvidence}</div>
                    </div>
                    
                    <div class="correlation-type">
                        <div class="correlation-type-title">üìù Text Evidence</div>
                        <div class="correlation-evidence">${correlation.textEvidence}</div>
                    </div>
                    
                    <div class="correlation-type">
                        <div class="correlation-type-title">üß† Contextual Evidence</div>
                        <div class="correlation-evidence">${correlation.contextualEvidence}</div>
                    </div>
                    
                    <div class="correlation-type">
                        <div class="correlation-type-title">‚è∞ Temporal Evidence</div>
                        <div class="correlation-evidence">${correlation.temporalEvidence}</div>
                    </div>
                    
                    <div class="correlation-type">
                        <div class="correlation-type-title">üîó Multimodal Fusion</div>
                        <div class="correlation-evidence">${correlation.multimodalFusion}</div>
                    </div>
                </div>
            `;

            container.appendChild(correlationCard);
        });
    }

    showDetailedEvidence() {
        if (!this.currentAnalysisResult) {
            alert('No analysis results available for detailed evidence view. Please run an analysis first.');
            return;
        }

        console.log('üîç Showing detailed evidence analysis...');

        this.displayDetailedEvidenceModal(this.currentAnalysisResult);

        const modal = document.getElementById('evidenceModal');
        if (modal) {
            modal.classList.remove('hidden');
        }
    }

    displayDetailedEvidenceModal(results) {
        const container = document.getElementById('detailedEvidence');
        if (!container) return;

        container.innerHTML = `
            <div class="detailed-evidence-section" style="margin-bottom: var(--space-24);">
                <h4>üìä Analysis Overview</h4>
                <div class="evidence-overview" style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: var(--space-12); background: var(--color-bg-1); padding: var(--space-16); border-radius: var(--radius-base);">
                    <div class="overview-metric" style="text-align: center;">
                        <strong>Overall Accuracy:</strong><br>${results.overallAccuracy}%
                    </div>
                    <div class="overview-metric" style="text-align: center;">
                        <strong>Transcript Confidence:</strong><br>${results.transcriptConfidence}%
                    </div>
                    <div class="overview-metric" style="text-align: center;">
                        <strong>Processing Time:</strong><br>${results.processingTime}s
                    </div>
                    <div class="overview-metric" style="text-align: center;">
                        <strong>Models Processed:</strong><br>${Object.keys(results.modelResults).length}
                    </div>
                </div>
            </div>
            
            <div class="detailed-evidence-section" style="margin-bottom: var(--space-24);">
                <h4>üé≠ Model-by-Model Evidence Analysis</h4>
                ${Object.entries(results.modelResults).map(([modelName, result]) => `
                    <div class="detailed-model-evidence" style="background: var(--color-surface); border: 1px solid var(--color-card-border); border-radius: var(--radius-lg); padding: var(--space-16); margin-bottom: var(--space-16);">
                        <h5 style="margin: 0 0 var(--space-12) 0; color: var(--color-primary);">${result.model.icon} ${modelName} Enhanced - ${result.confidence}% Confidence</h5>
                        
                        <div class="evidence-category" style="margin-bottom: var(--space-12);">
                            <h6 style="margin: 0 0 var(--space-6) 0; color: var(--color-text);">üé§ Audio Analysis Capabilities:</h6>
                            <p style="margin: 0 0 var(--space-8) 0; font-style: italic; color: var(--color-text-secondary);"><em>${result.model.audioAnalysis}</em></p>
                            <div class="evidence-items">
                                ${result.audioEvidence.map(evidence => `<div class="evidence-detail" style="background: var(--color-bg-1); padding: var(--space-8); border-radius: var(--radius-sm); margin-bottom: var(--space-4);">${evidence}</div>`).join('')}
                            </div>
                        </div>
                        
                        <div class="evidence-category" style="margin-bottom: var(--space-12);">
                            <h6 style="margin: 0 0 var(--space-6) 0; color: var(--color-text);">üìù Text Analysis Capabilities:</h6>
                            <p style="margin: 0 0 var(--space-8) 0; font-style: italic; color: var(--color-text-secondary);"><em>${result.model.textAnalysis}</em></p>
                            <div class="evidence-items">
                                ${result.textEvidence.map(evidence => `<div class="evidence-detail" style="background: var(--color-bg-1); padding: var(--space-8); border-radius: var(--radius-sm); margin-bottom: var(--space-4);">${evidence}</div>`).join('')}
                            </div>
                        </div>
                        
                        <div class="evidence-category" style="margin-bottom: var(--space-12);">
                            <h6 style="margin: 0 0 var(--space-6) 0; color: var(--color-text);">üîó Fusion Explanation:</h6>
                            <div class="fusion-detail" style="background: var(--color-bg-3); padding: var(--space-12); border-radius: var(--radius-sm); font-weight: var(--font-weight-medium);">${result.fusionExplanation}</div>
                        </div>
                        
                        <div class="evidence-category">
                            <h6 style="margin: 0 0 var(--space-8) 0; color: var(--color-text);">üéØ Top Detected Emotions:</h6>
                            <div class="emotion-details" style="display: flex; flex-direction: column; gap: var(--space-6);">
                                ${Object.entries(result.emotions)
                                    .sort(([,a], [,b]) => b - a)
                                    .slice(0, 3)
                                    .map(([emotion, confidence]) => `
                                        <div class="detailed-emotion" style="display: flex; justify-content: space-between; align-items: center; background: var(--color-bg-1); padding: var(--space-8); border-radius: var(--radius-sm);">
                                            <span>${this.emotionIcons[emotion]} ${emotion}</span>
                                            <span class="detailed-confidence" style="font-weight: var(--font-weight-bold); color: var(--color-primary);">${(confidence * 100).toFixed(1)}%</span>
                                        </div>
                                    `).join('')}
                            </div>
                        </div>
                    </div>
                `).join('')}
            </div>
            
            <div class="detailed-evidence-section">
                <h4>üîó Cross-Modal Evidence Summary</h4>
                <div style="background: var(--color-bg-1); padding: var(--space-16); border-radius: var(--radius-base);">
                    <p style="margin-bottom: var(--space-12);">This analysis demonstrates the power of multimodal emotion detection by combining voice characteristics and text content analysis across all 8 models. Each model contributes unique insights:</p>
                    <ul style="margin: 0; padding-left: var(--space-20);">
                        <li style="margin-bottom: var(--space-8);"><strong>Audio-focused models</strong> detect prosodic patterns, vocal stress, and acoustic features</li>
                        <li style="margin-bottom: var(--space-8);"><strong>Text-focused capabilities</strong> analyze semantic meaning, emotional keywords, and linguistic patterns</li>
                        <li style="margin-bottom: var(--space-8);"><strong>Fusion algorithms</strong> combine evidence from both modalities for enhanced accuracy</li>
                        <li style="margin-bottom: var(--space-8);"><strong>Temporal analysis</strong> tracks emotional progression over time</li>
                        <li><strong>Contextual understanding</strong> considers situational and conversational context</li>
                    </ul>
                </div>
            </div>
        `;
    }

    closeEvidenceModal() {
        const modal = document.getElementById('evidenceModal');
        if (modal) {
            modal.classList.add('hidden');
        }
    }

    exportCompleteAnalysis() {
        if (!this.currentAnalysisResult) {
            alert('No analysis results to export. Please run an analysis first.');
            return;
        }

        console.log('üìä Exporting complete multimodal analysis...');

        const exportData = {
            reportTitle: "Enhanced Multimodal Emotion AI - Complete Analysis Report",
            timestamp: this.currentAnalysisResult.timestamp.toISOString(),
            audioSource: this.currentAnalysisResult.audioSource,
            transcript: this.currentAnalysisResult.transcript,
            
            systemInfo: {
                modelArchitecture: "8-Model Enhanced Multimodal Fusion",
                analysisTypes: ["Voice Characteristics", "Text Content", "Cross-Modal Correlation"],
                overallAccuracy: this.currentAnalysisResult.overallAccuracy,
                transcriptConfidence: this.currentAnalysisResult.transcriptConfidence,
                processingTime: this.currentAnalysisResult.processingTime
            },
            
            transcriptAnalysis: {
                segments: this.currentAnalysisResult.transcriptSegments,
                totalSegments: this.currentAnalysisResult.transcriptSegments.length
            },
            
            modelResults: this.currentAnalysisResult.modelResults,
            emotionalTimeline: this.currentAnalysisResult.emotionalTimeline,
            evidenceCorrelation: this.currentAnalysisResult.evidenceCorrelation,
            
            methodology: {
                multimodalFusion: "Enhanced analysis combining voice characteristics and text content",
                audioAnalysis: "Prosody, tone, pitch, energy, breathing patterns, vocal stress analysis",
                textAnalysis: "Semantic meaning, emotional keywords, linguistic patterns, contextual analysis",
                temporalTracking: "Emotional progression timeline with trigger point detection",
                evidenceCorrelation: "Cross-modal validation between audio and text evidence"
            }
        };

        const dataStr = JSON.stringify(exportData, null, 2);
        const blob = new Blob([dataStr], { type: 'application/json' });
        const url = URL.createObjectURL(blob);

        const a = document.createElement('a');
        a.href = url;
        a.download = `enhanced-multimodal-emotion-analysis-${Date.now()}.json`;
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
        URL.revokeObjectURL(url);

        console.log('‚úÖ Complete multimodal analysis exported successfully');
    }

    addToHistory(analysisResult) {
        const historyEntry = {
            ...analysisResult,
            id: Date.now(),
            summary: {
                topEmotions: this.getTopEmotionsFromResults(analysisResult),
                modelCount: Object.keys(analysisResult.modelResults).length,
                hasTranscript: Boolean(analysisResult.transcript),
                analysisType: "Enhanced Multimodal"
            }
        };

        this.analysisHistory.unshift(historyEntry);

        if (this.analysisHistory.length > 15) {
            this.analysisHistory = this.analysisHistory.slice(0, 15);
        }

        this.updateHistoryDisplay();
        console.log('üìà Enhanced multimodal analysis added to history');
    }

    getTopEmotionsFromResults(results) {
        const emotionCounts = {};
        
        // Aggregate emotions from all models
        Object.values(results.modelResults).forEach(modelResult => {
            Object.entries(modelResult.emotions).forEach(([emotion, confidence]) => {
                emotionCounts[emotion] = (emotionCounts[emotion] || 0) + confidence;
            });
        });

        // Return top 3 emotions
        return Object.entries(emotionCounts)
            .sort(([,a], [,b]) => b - a)
            .slice(0, 3)
            .map(([emotion, totalConfidence]) => ({
                emotion,
                confidence: totalConfidence / Object.keys(results.modelResults).length
            }));
    }

    updateHistoryDisplay() {
        const historyList = document.getElementById('historyList');
        if (!historyList) return;

        if (this.analysisHistory.length === 0) {
            historyList.innerHTML = `
                <div class="empty-history">
                    <p>No previous analyses. Start by recording or uploading audio to experience the enhanced multimodal emotion AI system.</p>
                </div>
            `;
            return;
        }

        historyList.innerHTML = '';

        this.analysisHistory.forEach(analysis => {
            const historyItem = document.createElement('div');
            historyItem.className = 'history-item';

            const timestamp = analysis.timestamp.toLocaleString();
            const topEmotions = analysis.summary.topEmotions.slice(0, 3);

            historyItem.innerHTML = `
                <div class="history-main">
                    <div class="history-timestamp">${timestamp}</div>
                    <div class="history-mode">Enhanced Multimodal Analysis</div>
                    <div class="history-emotions">
                        ${topEmotions.map(({ emotion, confidence }) => `
                            <div class="history-emotion">
                                <span>${this.emotionIcons[emotion]}</span>
                                <span>${emotion} (${(confidence * 100).toFixed(0)}%)</span>
                            </div>
                        `).join('')}
                    </div>
                </div>
                <div class="history-metrics">
                    <div class="history-accuracy">${analysis.overallAccuracy}%</div>
                    <div class="history-features">
                        <div class="feature-badge">Multimodal</div>
                        ${analysis.transcript ? '<div class="feature-badge">Transcript</div>' : ''}
                        <div class="feature-badge">8 Models</div>
                    </div>
                </div>
            `;

            historyList.appendChild(historyItem);
        });
    }

    clearAnalysisHistory() {
        if (confirm('Are you sure you want to clear all analysis history?')) {
            this.analysisHistory = [];
            this.updateHistoryDisplay();
            console.log('üóëÔ∏è Analysis history cleared');
        }
    }

    exportAnalysisHistory() {
        if (this.analysisHistory.length === 0) {
            alert('No analysis history to export.');
            return;
        }

        const exportData = {
            exportTitle: "Enhanced Multimodal Emotion AI - Analysis History",
            exportTimestamp: new Date().toISOString(),
            totalAnalyses: this.analysisHistory.length,
            systemInfo: {
                modelArchitecture: "8-Model Enhanced Multimodal Architecture",
                analysisCapabilities: ["Voice Characteristics", "Text Content", "Cross-Modal Fusion"],
                overallAccuracy: this.overallAccuracy
            },
            analysisHistory: this.analysisHistory
        };

        const dataStr = JSON.stringify(exportData, null, 2);
        const blob = new Blob([dataStr], { type: 'application/json' });
        const url = URL.createObjectURL(blob);

        const a = document.createElement('a');
        a.href = url;
        a.download = `enhanced-multimodal-history-${Date.now()}.json`;
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
        URL.revokeObjectURL(url);

        console.log('üìã Enhanced multimodal analysis history exported successfully');
    }

    resetForNewAnalysis() {
        const multimodalResults = document.getElementById('multimodalResults');
        const liveTranscript = document.getElementById('liveTranscript');
        
        if (multimodalResults) multimodalResults.classList.add('hidden');
        if (liveTranscript) liveTranscript.classList.add('hidden');

        this.currentAnalysisResult = null;
        this.currentTranscript = "";
        this.transcriptSegments = [];

        // Reset recording state
        this.isRecording = false;
        if (this.recordingTimer) {
            clearInterval(this.recordingTimer);
            this.recordingTimer = null;
        }
        if (this.transcriptTimer) {
            clearInterval(this.transcriptTimer);
            this.transcriptTimer = null;
        }

        // Reset UI
        const recordBtn = document.getElementById('recordBtn');
        const recordText = document.getElementById('recordText');
        if (recordText) recordText.textContent = 'üéôÔ∏è Start Recording & Transcription';
        if (recordBtn) {
            recordBtn.classList.remove('btn--secondary');
            recordBtn.classList.add('btn--primary');
        }

        // Reset model status
        this.resetModelStatus();

        console.log('üîÑ System reset for new enhanced multimodal analysis');
    }

    handleAnalysisError() {
        console.error('Analysis failed');
        alert('An error occurred during analysis. Please try again.');
        this.resetForNewAnalysis();
    }
}

// Initialize the Enhanced Multimodal Emotion AI System
let enhancedEmotionAI;

document.addEventListener('DOMContentLoaded', () => {
    enhancedEmotionAI = new EnhancedMultimodalEmotionAI();
});